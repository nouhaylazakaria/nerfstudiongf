# Copyright 2022 the Regents of the University of California, Nerfstudio Team and contributors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.



from pathlib import Path
from typing import Dict, Literal

import numpy as np
import torch
from nerfstudio.data.dataparsers.base_dataparser import DataparserOutputs, Semantics
from nerfstudio.data.datasets.base_dataset import InputDataset
from nerfstudio.data.utils.data_utils import (
    get_depth_image_from_path,
    get_image_mask_tensor_from_path,
    get_semantics_and_mask_tensors_from_path,
)
from PIL import Image


class ngfDataset(InputDataset):
    """Dataset that returns image, depth lidar, monodepth, segmentation, normal.

    Args:
        dataparser_outputs: description of where and how to read input images.
    """

    exclude_batch_keys_from_device = InputDataset.exclude_batch_keys_from_device + ["semantics", "depth", "monodepth"]

    def __init__(self, dataparser_outputs: DataparserOutputs, scale_factor: float = 1.0):
        super().__init__(dataparser_outputs, scale_factor)
        # assert "semantics" in dataparser_outputs.metadata.keys() and isinstance(self.metadata["semantics"], Semantics)
        self.depth_filenames = self.metadata.get("depth_filenames")
        self.depth_unit_scale_factor = self.metadata.get("depth_unit_scale_factor")
        self.monodepth_filenames = self.metadata.get("monodepth_filenames")
        self.normal_filenames = self.metadata.get("normal_filenames")
        self.semantics: Semantics | any = self.metadata.get("semantics")
        self.instance_filenames = self.metadata.get("instance_filenames")

        if isinstance(self.metadata.get("semantics"), Semantics):
            self.mask_indices = torch.tensor(
                [self.semantics.classes.index(mask_class) for mask_class in self.semantics.mask_classes]
            ).view(1, 1, -1)

        # load lidar pts and l2w
        self.lidar_points = self.metadata.get("lidar_points_world")
        
        self.lidar2world = self.metadata.get("lidar2world")
        # self.lidar_dists = self.metadata.get("lidar_points_dists")
        self.points_2d = self.metadata.get("points_2d")
        # load dynamic object mask
        self.dynamic_object_mask = self.metadata.get("dynamic_object_mask")

    def get_metadata(self, data: Dict) -> Dict:
        batch = {}
        # mask = None
        height = int(self._dataparser_outputs.cameras.height[data["image_idx"]])
        width = int(self._dataparser_outputs.cameras.width[data["image_idx"]])

        if (
            hasattr(self, "semantics")
            and self.semantics is not None
            and hasattr(self.semantics, "filenames")
            and len(self.semantics.filenames) > 0
        ):
            filepath_semantics = self.semantics.filenames[data["image_idx"]]
            semantic_label, _ = get_semantics_and_mask_tensors_from_path(
                filepath=filepath_semantics, mask_indices=self.mask_indices, scale_factor=self.scale_factor
            )

            batch.update({"semantics": semantic_label.to(torch.int8)})

        if (
            hasattr(self, "instance_filenames")
            and self.instance_filenames is not None
            and len(self.instance_filenames) > 0
        ):
            filepath_instance = self.instance_filenames[data["image_idx"]]
            instance_label, _ = get_semantics_and_mask_tensors_from_path(
                filepath=filepath_instance, mask_indices=[], scale_factor=self.scale_factor
            )

            batch.update({"instance": instance_label.to(torch.int8)})

        if hasattr(self, "depth_filenames") and self.depth_filenames is not None and len(self.depth_filenames) > 0:
            scale_factor = self.depth_unit_scale_factor * self._dataparser_outputs.dataparser_scale
            filepath_depth = self.depth_filenames[data["image_idx"]]
            depth_image = get_depth_image_from_path(
                filepath=filepath_depth, height=height, width=width, scale_factor=scale_factor
            )
            batch.update({"depth": depth_image.to(torch.float32)})

        if (
            hasattr(self, "monodepth_filenames")
            and self.monodepth_filenames is not None
            and len(self.monodepth_filenames) > 0
        ):
            filepath_monodepth = self.monodepth_filenames[data["image_idx"]]
            monodepth_image = get_depth_image_from_path(
                filepath=filepath_monodepth, height=height, width=width, scale_factor=1.0
            )
            monodepth_image = (monodepth_image - monodepth_image.min()) / (
                monodepth_image.max() - monodepth_image.min()
            )
            monodepth_image = 1.0 - monodepth_image
            batch.update({"monodepth": monodepth_image.to(torch.float32)})

        if hasattr(self, "normal_filenames") and self.normal_filenames is not None and len(self.normal_filenames) > 0:
            filepath = self.normal_filenames[data["image_idx"]]
            normal_image = self.get_normal_image_from_path(path=filepath, model="DSINE")
            batch.update({"normal": normal_image})

        if hasattr(self, "lidar_points") and self.lidar_points is not None and len(self.lidar_points) > 0:
            lidar_pts = self.lidar_points[data["image_idx"]]
            batch.update({"lidar_points_world": lidar_pts})

        if hasattr(self, "lidar2world") and self.lidar2world is not None and len(self.lidar2world) > 0:
            l2w = self.lidar2world[data["image_idx"]]
            batch.update({"lidar2world": l2w})

        # if hasattr(self, "lidar_dists") and self.lidar_dists is not None and len(self.lidar_dists) > 0:
        #     dist = self.lidar_dists[data["image_idx"]]
        #     batch.update({"lidar_dists": dist})

        if hasattr(self, "points_2d") and self.points_2d is not None and len(self.points_2d) > 0:
            points_2d = self.points_2d[data["image_idx"]]
            batch.update({"points_2d": points_2d})

        if (
            hasattr(self, "dynamic_object_mask")
            and self.dynamic_object_mask is not None
            and len(self.dynamic_object_mask) > 0
        ):
            filepath = self.dynamic_object_mask[data["image_idx"]]
            dynamic_object_mask = get_image_mask_tensor_from_path(filepath, scale_factor=self.scale_factor)
            batch.update({"dynamic_object_mask": ~dynamic_object_mask})  # object mask file where == 0

        # deactivate mask for now
        # if "mask" in data.keys():
        #     mask = mask & data["mask"]
        # if mask is not None:
        #     batch.update({"mask": mask})
        return batch

    def get_normal_image_from_path(self, path, model: Literal["DSINE", "StableNormal"] = "DSINE"):
        """Helper function to load normal data

        Args:
            path: path to normal file
            model: model that generated normal map
                   DSINE outputs        [3, H, W]
                   StableNormal outputs [3, W, H]
        """
        if path.suffix == ".png":
            normal_map = np.array(Image.open(path), dtype="uint8")[..., :3]
        else:
            normal_map = np.load(path)
            normal_map = 2.0 * normal_map - 1.0  # [0, 1] => [-1, 1]

            if model == "DSINE":
                normal_map = normal_map.transpose(1, 2, 0)
                # convert from outward to inward
                normal_map[..., 0] = -normal_map[..., 0]  # just convert x direction
            elif model == "StableNormal":
                normal_map = normal_map.transpose(2, 1, 0)
                normal_map = normal_map @ np.diag(np.array([1, -1, -1]))  # convert YZ direction

            normal_map = (normal_map + 1.0) / 2.0  # [-1, 1] => [0, 1]

        normal_map = torch.from_numpy(normal_map.astype("float32")).float()  # already in [0, 1]

        return normal_map





if __name__ == "__main__":
    from pathlib import Path
    from nerfstudio.data.dataparsers.ngfdataparser import ngfDataParserConfig,ngfDataParser
   

    data_path = Path(
        "/data/dev/wod_export/experiment_lidar/sequence_141184_refined"
        # "/data/dev/lidar_pts_rendering/1758724094753801109_1251_037_1271_037_start0_end20_static-lidar_objects_depth_normal_dyn-obj-mask_undistord_images_cameras_1"
    )
    cfg = ngfDataParserConfig(data=data_path)
    autoscene_data = ngfDataParser(config=cfg)

    do = autoscene_data._generate_dataparser_outputs("val")

    autoscene_dataset = ngfDataset(do)
    lidar_pts_world = autoscene_dataset.lidar_points
    print(do)

